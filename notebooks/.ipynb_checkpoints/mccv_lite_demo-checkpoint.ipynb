{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCCV Lite Demo Notebook\n",
        "\n",
        "This notebook demonstrates the **end-to-end MCCV prototype** using the **lite (pure-Python)** implementation:\n",
        "\n",
        "- Generate **multi-level synthetic data** (beneficiary → diagnosis → multi-modality evidence)\n",
        "- Score **clinical coherence** (0 to 1)\n",
        "- Compare **fraud vs. non-fraud** cases (ground truth from the generator)\n",
        "- Produce a **human-readable audit report** for a flagged case\n",
        "\n",
        "> Note: This uses `mccv.lite` so it runs even when `numpy` / `pandas` are unavailable or unstable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from statistics import mean\n",
        "\n",
        "# Ensure we can import the local package\n",
        "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from mccv.lite.synthetic_generator import MedicareSyntheticGeneratorLite, HCC_DEFINITIONS_LITE\n",
        "from mccv.lite.knowledge_graph import ClinicalKnowledgeGraphLite\n",
        "from mccv.lite.rule_based_scorer import RuleBasedCoherenceScorerLite\n",
        "from mccv.lite.audit_report import AuditReportGeneratorLite\n",
        "\n",
        "print(\"Imports OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Generate synthetic data (multi-level)\n",
        "gen = MedicareSyntheticGeneratorLite(\n",
        "    n_beneficiaries=500,\n",
        "    fraud_rate=0.18,\n",
        "    seed=42,\n",
        ")\n",
        "data = gen.generate()\n",
        "\n",
        "meta = (data.get(\"meta\") or [{}])[0]\n",
        "print(\"Measurement window:\", meta.get(\"measurement_start_date\"), \"→\", meta.get(\"measurement_end_date\"))\n",
        "print(\"Tables:\", sorted(data.keys()))\n",
        "print(\"Beneficiaries:\", len(data[\"beneficiaries\"]))\n",
        "print(\"Diagnosis records:\", len(data[\"diagnosis_records\"]))\n",
        "print(\"Pharmacy claims:\", len(data[\"pharmacy_claims\"]))\n",
        "print(\"Lab claims:\", len(data[\"lab_claims\"]))\n",
        "print(\"Specialist visits:\", len(data[\"specialist_visits\"]))\n",
        "print(\"Procedure claims:\", len(data[\"procedure_claims\"]))\n",
        "print(\"HRA records:\", len(data[\"hra_records\"]))\n",
        "print(\"Labels:\", len(data[\"labels\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Build knowledge graph + score coherence (rule-based)\n",
        "kg = ClinicalKnowledgeGraphLite(hcc_definitions={k: v.__dict__ for k, v in HCC_DEFINITIONS_LITE.items()})\n",
        "weights_path = os.path.join(PROJECT_ROOT, \"mccv\", \"configs\", \"hcc_weights.yaml\")\n",
        "\n",
        "scorer = RuleBasedCoherenceScorerLite(\n",
        "    knowledge_graph=kg,\n",
        "    weights_config_path=weights_path,\n",
        "    measurement_start_date=meta.get(\"measurement_start_date\"),\n",
        "    measurement_end_date=meta.get(\"measurement_end_date\"),\n",
        ")\n",
        "\n",
        "preds = scorer.score_dataset(data)  # list[dict]\n",
        "labels = data[\"labels\"]\n",
        "label_by_key = {(l[\"beneficiary_id\"], l[\"hcc_code\"]): l for l in labels}\n",
        "\n",
        "# Merge predictions with ground truth labels\n",
        "joined = []\n",
        "for p in preds:\n",
        "    l = label_by_key.get((p[\"beneficiary_id\"], p[\"hcc_code\"]), {})\n",
        "    joined.append({**p, **{\"is_fraudulent\": l.get(\"is_fraudulent\", False), \"fraud_type\": l.get(\"fraud_type\", \"\")}})\n",
        "\n",
        "# Summary stats\n",
        "fraud_scores = [r[\"coherence_score\"] for r in joined if r[\"is_fraudulent\"]]\n",
        "legit_scores = [r[\"coherence_score\"] for r in joined if not r[\"is_fraudulent\"]]\n",
        "print(\"Scored rows:\", len(joined))\n",
        "print(\"Mean coherence (fraud):\", round(mean(fraud_scores), 3) if fraud_scores else None)\n",
        "print(\"Mean coherence (non-fraud):\", round(mean(legit_scores), 3) if legit_scores else None)\n",
        "\n",
        "# Breakdown of high-risk flags by ground-truth fraud_type\n",
        "high_risk = [r for r in joined if r[\"coherence_score\"] < scorer.high_risk_threshold]\n",
        "counts = {}\n",
        "for r in high_risk:\n",
        "    if not r[\"is_fraudulent\"]:\n",
        "        continue\n",
        "    ft = r.get(\"fraud_type\") or \"unknown\"\n",
        "    counts[ft] = counts.get(ft, 0) + 1\n",
        "\n",
        "print(f\"High-risk (score < {scorer.high_risk_threshold:.2f}):\", len(high_risk), \"/\", len(joined))\n",
        "if counts:\n",
        "    print(\"High-risk fraud breakdown:\")\n",
        "    for k in sorted(counts.keys()):\n",
        "        print(f\"  - {k}: {counts[k]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Show example cases: fraud vs non-fraud\n",
        "\n",
        "def pick_examples(rows, *, is_fraudulent: bool, coherence_min=None, coherence_max=None, n=5):\n",
        "    out = []\n",
        "    for r in sorted(rows, key=lambda x: x[\"coherence_score\"]):\n",
        "        if bool(r[\"is_fraudulent\"]) != bool(is_fraudulent):\n",
        "            continue\n",
        "        s = r[\"coherence_score\"]\n",
        "        if coherence_min is not None and s < coherence_min:\n",
        "            continue\n",
        "        if coherence_max is not None and s > coherence_max:\n",
        "            continue\n",
        "        out.append(r)\n",
        "        if len(out) >= n:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "fraud_low = pick_examples(joined, is_fraudulent=True, coherence_max=scorer.high_risk_threshold, n=5)\n",
        "legit_high = pick_examples(joined, is_fraudulent=False, coherence_min=0.70, n=5)\n",
        "\n",
        "print(\"\\nFraud examples (low coherence):\")\n",
        "for r in fraud_low:\n",
        "    print(\n",
        "        f\"- {r['beneficiary_id']} {r['hcc_code']} fraud_type={r.get('fraud_type')} \"\n",
        "        f\"score={r['coherence_score']:.2f} (ph={r['pharmacy_score']:.2f}, lab={r['lab_score']:.2f}, spec={r['specialist_score']:.2f}, proc={r['procedure_score']:.2f})\"\n",
        "    )\n",
        "\n",
        "print(\"\\nNon-fraud examples (high coherence):\")\n",
        "for r in legit_high:\n",
        "    print(\n",
        "        f\"- {r['beneficiary_id']} {r['hcc_code']} score={r['coherence_score']:.2f} \"\n",
        "        f\"(ph={r['pharmacy_score']:.2f}, lab={r['lab_score']:.2f}, spec={r['specialist_score']:.2f}, proc={r['procedure_score']:.2f})\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Generate an audit report for one fraud case (with evidence)\n",
        "\n",
        "# build one combined claims list\n",
        "all_claims = []\n",
        "for k in [\"pharmacy_claims\", \"lab_claims\", \"specialist_visits\", \"procedure_claims\"]:\n",
        "    all_claims.extend(data.get(k, []))\n",
        "\n",
        "if fraud_low:\n",
        "    sample = fraud_low[0]\n",
        "    report_gen = AuditReportGeneratorLite(kg)\n",
        "    weights_for_report = scorer.get_weights_for_report(sample[\"hcc_code\"])\n",
        "\n",
        "    report = report_gen.generate_report(\n",
        "        beneficiary_id=sample[\"beneficiary_id\"],\n",
        "        hcc_code=sample[\"hcc_code\"],\n",
        "        coherence_score=sample[\"coherence_score\"],\n",
        "        all_claims=all_claims,\n",
        "        diagnosis_origin={\"source\": \"Synthetic\", \"date\": \"(see diagnosis_records)\", \"provider\": \"Synthetic\"},\n",
        "        weights=weights_for_report,\n",
        "    )\n",
        "\n",
        "    print(report_gen.format_report(report))\n",
        "else:\n",
        "    print(\"No low-coherence fraud examples found. Try increasing fraud_rate.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
